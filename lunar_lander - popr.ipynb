{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ustawienie hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import warnings\n",
    "\n",
    "# Ustawienia dla reprodukowalności\n",
    "SEED = 42\n",
    "\n",
    "OPTIMIZE_WITH_HARDWARE = True\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if OPTIMIZE_WITH_HARDWARE:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f'Wybrano urządzenie: MPS')\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f'Wybrano urządzenie: GPU z obsługą CUDA')\n",
    "        print(f'Nazwa urządzenia CUDA: {torch.cuda.get_device_name()}')\n",
    "        print(f'Liczba urządzeń CUDA: {torch.cuda.device_count()}')\n",
    "        print(f'Indeks urządzenia CUDA: {torch.cuda.current_device()}')\n",
    "else:\n",
    "    print(f'Wybrano urządzenie: CPU')\n",
    "\n",
    "# Ustawienie ziaren losowych\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ogólne przedstawienie problemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja środowiska\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "state = env.reset()\n",
    "if isinstance(state, tuple):\n",
    "    state = state[0]\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action = env.action_space.sample()  # Losowa akcja\n",
    "    result = env.step(action)\n",
    "    if len(result) == 5:\n",
    "        next_state, reward, terminated, truncated, info = result\n",
    "        done = terminated or truncated\n",
    "    else:\n",
    "        next_state, reward, done, info = result\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co zwraca środowisko?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stan - wektor opisujący aktualne położenie i dynamikę lądownika.\n",
    "- pozycja pozioma lądownika (x);\n",
    "- pozycja pionowa lądownika (y)\n",
    "- prędkość pozioma lądownika\n",
    "- prędkość pionowa lądownika\n",
    "- kąt nachylenia lądownika\n",
    "- prędkość kątowa lądownika\n",
    "- czy noga nr 1 (lewa) lądownika dotyka podłoża\n",
    "- czy noga nr 2 (prawa) lądownika dotyka podłoża"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ile akcji może wykonać agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ilość możliwych akcji: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Brak działania\n",
    "2. Uruchomiony główny silnik\n",
    "3. Uruchomiony lewy silnik\n",
    "4. Uruchomiony peawy silnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametry\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99  # Współczynnik dyskontowania przyszłych nagród\n",
    "EPSILON_START = 1.0  # Początkowa wartość współczynnika eksploracji\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995  # Współczynnik zmniejszania wartości epsilon (na epizod)\n",
    "MEMORY_SIZE = 100000  # Maksymalny rozmiar pamięci przechowującej przejścia\n",
    "TRAIN_START = 1000  # Rozpoczęcie uczenia po zebraniu danej liczby doświadczeń\n",
    "TARGET_UPDATE_STEPS = 1000  # Aktualizacja sieci docelowej co określoną liczbę kroków\n",
    "MAX_EPISODES = 500\n",
    "MAX_STEPS = 1000  # Maksymalna liczba kroków w epizodzie\n",
    "\n",
    "# Hiperparametry do przetestowania\n",
    "learning_rates = [1e-2, 1e-4, 1e-6]\n",
    "discount_factors = [0.8, 0.9, 0.99]\n",
    "epsilon_decays = [0.99910, 0.99941, 0.99954, 0.99973, 0.99987]\n",
    "\n",
    "# Liczba epizodów\n",
    "EPISODES = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaimportowanie niezbędnych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struktura sieci głębokiej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otrzymujemy stan w postaci wektora ośmiu wyżej wymienionych parametrów. Definiujemy 3 liniowe warstwy, tzn. przetwrzające dane liniowo, realizujące sume ważoną.\n",
    "- Pierwsza warstwa przyjmuje stan w którym znajduje się łazik (state_size), rozszerza na 128 parametrów.\n",
    "- Druga warstwa przyjmuje 128 parametrów i na wyjściu ma 128 parametrów.\n",
    "- Trzecia warstwa która będzie podawać wartość Q dla każdej możliwej do podjęcia akcji przyjmuje 128 parametrów i zwraca 4 wyjścia (action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)  # Warstwa wejściowa\n",
    "        self.fc2 = nn.Linear(256, 256)        # Warstwa ukryta\n",
    "        self.fc3 = nn.Linear(256, output_size)  # Warstwa wyjściowa\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_values = self.fc3(x)  # Q-wartości dla każdej akcji\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja agenta sieci DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.model = DQNetwork(input_size, output_size).to(device)\n",
    "        self.target_model = DQNetwork(input_size, output_size).to(device)\n",
    "        self.update_target_model()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss_function = nn.SmoothL1Loss()\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.output_size)\n",
    "        state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < TRAIN_START:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "\n",
    "        # Double DQN\n",
    "        next_actions = self.model(next_states).argmax(1).unsqueeze(1)\n",
    "        next_q_values = self.target_model(next_states).gather(1, next_actions).detach()\n",
    "\n",
    "        target_q_values = rewards + (GAMMA * next_q_values * (1 - dones))\n",
    "\n",
    "        loss = self.loss_function(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Obcinanie gradientów\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.total_steps += 1\n",
    "        if self.total_steps % TARGET_UPDATE_STEPS == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon *= EPSILON_DECAY\n",
    "        else:\n",
    "            self.epsilon = EPSILON_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja pętli treningowej modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja pętli treningowej modelu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "agent = DQNAgent(input_size, output_size)\n",
    "\n",
    "\n",
    "episodes = MAX_EPISODES\n",
    "rewards_history = []\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done and step < MAX_STEPS:\n",
    "        action = agent.act(state)\n",
    "        result = env.step(action)\n",
    "        if len(result) == 5:\n",
    "            next_state, reward, terminated, truncated, info = result\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_state, reward, done, info = result\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        agent.replay()\n",
    "        step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "    rewards_history.append(total_reward)\n",
    "\n",
    "    print(f\"Epizod {episode + 1}/{episodes}, Nagroda: {total_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': agent.model.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon,\n",
    "}, 'dqn_lunar_lander.pth')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicja pętli treningowej - zależność od różnych wartości alpha i gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja pętli treningowej modelu\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "agent = DQNAgent(input_size, output_size)\n",
    "\n",
    "\n",
    "episodes = MAX_EPISODES\n",
    "rewards_history = []\n",
    "total_steps = 0\n",
    "\n",
    "# Grid search dla różnych wartości hyperparametrów\n",
    "results = []\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    for gamma in discount_factors:\n",
    "        for epsilon_decay in epsilon_decays:\n",
    "            # Kod dla treningu agenta\n",
    "            agent = DQNAgent(input_size, output_size)\n",
    "            agent.optimizer = torch.optim.Adam(agent.model.parameters(), lr=alpha)\n",
    "            agent.gamma = gamma\n",
    "            agent.epsilon_decay = epsilon_decay\n",
    "\n",
    "            rewards_history = []\n",
    "\n",
    "            for episode in range(EPISODES):  # Liczba epizodów\n",
    "                state = env.reset()\n",
    "                if isinstance(state, tuple):\n",
    "                    state = state[0]\n",
    "                total_reward = 0\n",
    "                done = False\n",
    "\n",
    "                while not done:\n",
    "                    action = agent.act(state)\n",
    "                    result = env.step(action)\n",
    "                    if len(result) == 5:\n",
    "                        next_state, reward, terminated, truncated, info = result\n",
    "                        done = terminated or truncated\n",
    "                    else:\n",
    "                        next_state, reward, done, info = result\n",
    "\n",
    "                    agent.memorize(state, action, reward, next_state, done)\n",
    "                    agent.replay()\n",
    "                    state = next_state\n",
    "                    total_reward += reward\n",
    "\n",
    "                agent.decay_epsilon()\n",
    "                rewards_history.append(total_reward)\n",
    "\n",
    "            # Zapisz wyniki dla tej kombinacji hyperparametrów\n",
    "            results.append({\n",
    "                'alpha': alpha,\n",
    "                'gamma': gamma,\n",
    "                'epsilon_decay': epsilon_decay,\n",
    "                'rewards': rewards_history\n",
    "            })\n",
    "# Grupowanie danych dla różnych epsilon_decay\n",
    "epsilon_decay_results = {}\n",
    "for result in results:\n",
    "    key = result['epsilon_decay']\n",
    "    if key not in epsilon_decay_results:\n",
    "        epsilon_decay_results[key] = []\n",
    "    epsilon_decay_results[key].append(result['rewards'])\n",
    "\n",
    "# Grupowanie danych dla learning rate i gamma\n",
    "learning_rate_gamma_results = {}\n",
    "for result in results:\n",
    "    key = (result['alpha'], result['gamma'])\n",
    "    if key not in learning_rate_gamma_results:\n",
    "        learning_rate_gamma_results[key] = []\n",
    "    learning_rate_gamma_results[key].append(result['rewards'])\n",
    "\n",
    "# Zapis wyników do pliku JSON\n",
    "import json\n",
    "\n",
    "with open('results2.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykresy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykresy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "window_size = 10\n",
    "moving_avg = np.convolve(rewards_history, np.ones(window_size)/window_size, mode='valid')\n",
    "x = np.arange(len(moving_avg))\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x, moving_avg, label=f\"Średnia ruchoma (okno={window_size})\")\n",
    "plt.title(\"Nagroda na epizod\")\n",
    "plt.xlabel(\"Epizod\")\n",
    "plt.ylabel(\"Nagroda\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from gymnasium.envs.box2d import LunarLander\n",
    "\n",
    "# Rejestracja LunarLander-v2\n",
    "gym.register(\n",
    "    id='LunarLander-v2',\n",
    "    entry_point='gymnasium.envs.box2d:LunarLander',\n",
    "    max_episode_steps=1000,\n",
    "    reward_threshold=200,\n",
    ")\n",
    "\n",
    "# Inicjalizacja urządzenia\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Ustawienia\n",
    "episode_count = 5\n",
    "video_folder = 'videos'  # Folder na nagrania\n",
    "os.makedirs(video_folder, exist_ok=True)  # Tworzenie folderu, jeśli nie istnieje\n",
    "\n",
    "# Stwórz środowisko z poprawnym render_mode\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "# Załaduj wytrenowany model\n",
    "model = DQNetwork(input_size, output_size).to(device)\n",
    "checkpoint = torch.load('dqn_lunar_lander.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Ustaw epsilon na zero (choć nie jest używany podczas testowania)\n",
    "epsilon = 0.0\n",
    "\n",
    "# Testuj model i nagrywaj epizody\n",
    "for episode in range(episode_count):\n",
    "    # Wrapper RecordVideo dla każdego epizodu\n",
    "    episode_video_folder = os.path.join(video_folder, f\"episode_{episode + 1}\")\n",
    "    os.makedirs(episode_video_folder, exist_ok=True)\n",
    "    env = RecordVideo(env, video_folder=episode_video_folder, name_prefix=f\"lunar_lander_ep{episode + 1}\", episode_trigger=lambda x: True)\n",
    "\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        result = env.step(action)\n",
    "        if len(result) == 5:\n",
    "            next_state, reward, terminated, truncated, info = result\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_state, reward, done, info = result\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Epizod {episode + 1}: Łączna nagroda: {total_reward:.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy cd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funkcja do obliczania średniej ruchomej\n",
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Parametr okna średniej ruchomej\n",
    "window_size = 50\n",
    "\n",
    "# Wczytanie danych z pliku JSON\n",
    "file_path = 'results1.json'  # Podaj ścieżkę do pliku JSON\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Grupowanie danych według learning_rate i gamma\n",
    "learning_rate_gamma_results = {}\n",
    "e_decay_results = {}\n",
    "for entry in data:\n",
    "    alpha, gamma, e_decay = entry[\"alpha\"], entry[\"gamma\"], entry.get(\"epsilon_decay\", None)\n",
    "\n",
    "    # Grupowanie według alpha i gamma\n",
    "    key = (alpha, gamma)\n",
    "    if key not in learning_rate_gamma_results:\n",
    "        learning_rate_gamma_results[key] = []\n",
    "    learning_rate_gamma_results[key].append(entry[\"rewards\"])\n",
    "\n",
    "    # Grupowanie według epsilon_decay\n",
    "    if e_decay is not None:\n",
    "        if e_decay not in e_decay_results:\n",
    "            e_decay_results[e_decay] = []\n",
    "        e_decay_results[e_decay].append(entry[\"rewards\"])\n",
    "\n",
    "# Diagram 1: Wpływ learning rate (α) i gamma (γ) na nagrody agenta\n",
    "plt.figure(figsize=(12, 6))\n",
    "for (alpha, gamma), rewards_list in learning_rate_gamma_results.items():\n",
    "    mean_rewards = np.mean(rewards_list, axis=0)\n",
    "    smoothed_rewards = moving_average(mean_rewards, window_size)\n",
    "    smoothed_episodes = range(1, len(smoothed_rewards) + 1)\n",
    "\n",
    "    plt.plot(smoothed_episodes, smoothed_rewards, label=f'α={alpha}, γ={gamma}')\n",
    "\n",
    "plt.title('Wpływ α (learning rate) i γ (discount factor) na nagrody agenta (średnia ruchoma)')\n",
    "plt.xlabel('Epizody')\n",
    "plt.ylabel('Nagroda skumulowana')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Diagram 2: Wpływ epsilon-decay na nagrody agenta\n",
    "plt.figure(figsize=(12, 6))\n",
    "for e_decay, rewards_list in e_decay_results.items():\n",
    "    mean_rewards = np.mean(rewards_list, axis=0)\n",
    "    smoothed_rewards = moving_average(mean_rewards, window_size)\n",
    "    smoothed_episodes = range(1, len(smoothed_rewards) + 1)\n",
    "\n",
    "    plt.plot(smoothed_episodes, smoothed_rewards, label=f'ε-decay = {e_decay}')\n",
    "\n",
    "plt.title('Wpływ ε-decay na nagrody agenta (średnia ruchoma)')\n",
    "plt.xlabel('Epizody')\n",
    "plt.ylabel('Nagroda skumulowana')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Diagram 3: Wpływ learning rate (α) przy stałym gamma = 0.9\n",
    "plt.figure(figsize=(12, 6))\n",
    "for (alpha, gamma), rewards_list in learning_rate_gamma_results.items():\n",
    "    if gamma == 0.9:  # Stały discount factor\n",
    "        mean_rewards = np.mean(rewards_list, axis=0)\n",
    "        smoothed_rewards = moving_average(mean_rewards, window_size)\n",
    "        smoothed_episodes = range(1, len(smoothed_rewards) + 1)\n",
    "\n",
    "        plt.plot(smoothed_episodes, smoothed_rewards, label=f'α={alpha}')\n",
    "\n",
    "plt.title('Wpływ learning rate (α) na nagrody agenta (γ = 0.9)')\n",
    "plt.xlabel('Epizody')\n",
    "plt.ylabel('Nagroda skumulowana')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Diagram 4: Wpływ discount factor (γ) przy stałym learning rate = 1e-4\n",
    "plt.figure(figsize=(12, 6))\n",
    "for (alpha, gamma), rewards_list in learning_rate_gamma_results.items():\n",
    "    if alpha == 1e-4:  # Stały learning rate\n",
    "        mean_rewards = np.mean(rewards_list, axis=0)\n",
    "        smoothed_rewards = moving_average(mean_rewards, window_size)\n",
    "        smoothed_episodes = range(1, len(smoothed_rewards) + 1)\n",
    "\n",
    "        plt.plot(smoothed_episodes, smoothed_rewards, label=f'γ={gamma}')\n",
    "\n",
    "plt.title('Wpływ discount factor (γ) na nagrody agenta (α = 1e-4)')\n",
    "plt.xlabel('Epizody')\n",
    "plt.ylabel('Nagroda skumulowana')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
