{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ustawienie hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: GPU with CUDA support\n",
      "CUDA device name: Quadro M1200\n",
      "CUDA device count: 1\n",
      "CUDA device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "OPTIMIZE_WITH_HARDWARE = True\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if OPTIMIZE_WITH_HARDWARE:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f'Selected device: MPS')\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f'Selected device: GPU with CUDA support')\n",
    "        print(f'CUDA device name: {torch.cuda.get_device_name()}')\n",
    "        print(f'CUDA device count: {torch.cuda.device_count()}')\n",
    "        print(f'CUDA device index: {torch.cuda.current_device()}')\n",
    "else:\n",
    "    print(f'Selected device: CPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ogólne przedstawienie problemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "termined = False\n",
    "truncated = False\n",
    "\n",
    "while not (termined or truncated):\n",
    "    action = 1\n",
    "    obs, reward, terminated, truncated, info  = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co zwraca środowisko?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pozycja pozioma lądownika (x);\n",
    "- pozycja pionowa lądownika (y)\n",
    "- prędkość pozioma lądownika\n",
    "- prędkość pionowa lądownika\n",
    "- kąt nachylenia lądownika\n",
    "- prędkość kątowa lądownika\n",
    "- czy noga nr 1 (lewa) lądownika dotyka podłoża\n",
    "- czy noga nr 2 (prawa) lądownika dotyka podłoża"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ile akcji może wykonać agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilość możliwych akcji: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Ilość możliwych akcji: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Brak działania\n",
    "2. Uruchomiony główny silnik\n",
    "3. Uruchomiony lewy silnik\n",
    "4. Uruchomiony peawy silnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struktura sieci głębokiej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otrzymujemy stan w postaci wektora ośmiu wyżej wymienionych parametrów. Definiujemy 3 liniowe warstwy, tzn. przetwrzające dane liniowo, realizujące sume ważoną.\n",
    "- Pierwsza warstwa przyjmuje stan w którym znajduje się łazik (state_size), rozszerza na 128 parametrów.\n",
    "- Druga warstwa przyjmuje 128 parametrów i na wyjściu ma 128 parametrów.\n",
    "- Trzecia warstwa która będzie podawać wartość Q dla każdej możliwej do podjęcia akcji przyjmuje 128 parametrów i zwraca 4 wyjścia (action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn # importujemy moduł nn z biblioteki torch\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.nn.leaky_relu(self.fc1(state), negative_slope=0.01)\n",
    "        x = torch.nn.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
    "        x = self.fc3(state)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja agenta sieci DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque # impor kolejki deque z modułu collections\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparametry sieci neuronowej\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.input_size = input_size # ilość wejść, ilo informacji dot. stanu środowiska\n",
    "        self.output_size = output_size # ilość wyjść, ilość możliwych akcji\n",
    "        self.discount_factor = 0.99 # współczynnik dyskontujący\n",
    "        self.epsilon_greedy = 1.0 # początkowya wartość losowego wyboru akcji\n",
    "        self.epsilon_min = 0.01 # minimalna wartość losowego wyboru akcji\n",
    "        self.epsilon_greedy_decay = 0.995 # współczynnik zmniejszający wartość losowego wyboru akcji (co iteracje o 5%)\n",
    "        self.train_start = 500 # ilość próbek w pamięci agenta, po której rozpoczynamy uczenie\n",
    "        self.memory = deque(maxlen=1000) # pamięć agenta\n",
    "\n",
    "        self.model = DQNetwork(input_size, output_size).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done): \n",
    "        '''metoda dodająca informacje do pamięci agenta'''\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        '''wybiera akcję na podstawie stanu, jeśli model zwróci wartość większą niż epsilon_greedy,\n",
    "        to wybieramy akcję o największej wartości, w przeciwnym wypadku wybieramy losową akcję. unsqueeze(0) dodaje dodatkowy wymiar [BATCH_SIZE, input_size].\n",
    "        Pytorch operuje na tensorach, który w pierwszym wymiarze zawiera informacje i ilość paczek, następnie dane treningowe.\n",
    "        \"unsqueeze(0)\" dodaje dodatkowy wymiar, który jest wymagany przez model, mimo że jest jedna paczka w funkcji.'''\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon_greedy:\n",
    "            return random.randrange(self.output_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values_predicted = self.model(state)\n",
    "            return torch.argmax(q_values_predicted).item()\n",
    "\n",
    "    def replay(self):\n",
    "        '''metoda ucząca model na podstawie próbek z pamięci agenta'''\n",
    "        total_mse_loss = 0\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        data_batch = random.sample(self.memory, BATCH_SIZE) # losujemy próbki z pamięci agenta\n",
    "\n",
    "        for state, action, reward, next_state, done in data_batch:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device) #następny stan który zwróci środowisko dla danego doświdczenia\n",
    "            reward = torch.FloatTensor([reward]).to(device)\n",
    "            discount_reward = reward\n",
    "            if not done:\n",
    "                discount_reward = reward + torch.max(self.model(next_state)) * self.discount_factor # obliczamy zniżone nagrody\n",
    "            \n",
    "            dqnprediction = self.model(state)\n",
    "            true_reward = dqnprediction.clone() # klonujemy wartość predykcji\n",
    "            true_reward[action] = discount_reward # nadpisujemy wartość nagrody dla danej akcji\n",
    "\n",
    "            loss = self.loss_function(dqnprediction, true_reward)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() # aktualizujemy wagi modelu\n",
    "            total_mse_loss += loss.item()\n",
    "\n",
    "        if self.epsilon_greedy > self.epsilon_min:\n",
    "            self.epsilon_greedy *= self.epsilon_greedy_decay # zmniejszamy wartość losowego wyboru akcji\n",
    "        \n",
    "        return total_mse_loss/BATCH_SIZE # zwracamy średni błąd kwadratowy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja pętli treningowej modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_mse_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     total_mse_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mse_loss\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m---> 34\u001b[0m     AVG_MSE_LOSS \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_mse_loss\u001b[49m\u001b[38;5;241m/\u001b[39mstep_counter \u001b[38;5;28;01mif\u001b[39;00m step_counter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpizod: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Nagroda: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Krok: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mepsilon_greedy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAVG_MSE_LOSS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_mse_loss' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode='human') # inicjalizacja środowiska\n",
    "input_size = env.observation_space.shape[0] # rozmir informacji o stanie zwracanym przez środowisko\n",
    "output_size = env.action_space.n # ilość możliwych akcji\n",
    "agent = DQNAgent(input_size, output_size) # inicjalizacja agenta\n",
    "\n",
    "# Śledzenie postępu\n",
    "rewards_history = []\n",
    "epsilon_history = []\n",
    "loss_history = []\n",
    "\n",
    "episodes = 1000\n",
    "for episode in range(episodes): # pętla ucząca agenta\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_counter = 0\n",
    "    total_mse_loss = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        step_counter += 1\n",
    "\n",
    "        mse_loss = agent.replay()\n",
    "        if mse_loss is not None:\n",
    "            total_mse_loss += mse_loss\n",
    "        if done:\n",
    "            AVG_MSE_LOSS = total_mse_loss/step_counter if step_counter > 0 else 0\n",
    "            print(f\"Episode: {episode+1}/{episodes}, Reward: {total_reward:.2f}, \n",
    "                  Epsilon: {agent.epsilon_greedy:.3f}, MSE: {total_mse_loss:.3f} \n",
    "                  Loss: {math.sqrt(total_mse_loss):.2f}\")\n",
    "            rewards_history.append(total_reward) # zapisujemy dane o postępie\n",
    "            epsilon_history.append(agent.epsilon_greedy)\n",
    "            loss_history.append(AVG_MSE_LOSS)\n",
    "            break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test2137",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
