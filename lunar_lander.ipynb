{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ustawienie hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: GPU with CUDA support\n",
      "CUDA device name: Quadro M1200\n",
      "CUDA device count: 1\n",
      "CUDA device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "OPTIMIZE_WITH_HARDWARE = True\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if OPTIMIZE_WITH_HARDWARE:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f'Selected device: MPS')\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f'Selected device: GPU with CUDA support')\n",
    "        print(f'CUDA device name: {torch.cuda.get_device_name()}')\n",
    "        print(f'CUDA device count: {torch.cuda.device_count()}')\n",
    "        print(f'CUDA device index: {torch.cuda.current_device()}')\n",
    "else:\n",
    "    print(f'Selected device: CPU')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ogólne przedstawienie problemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "env.reset()\n",
    "\n",
    "termined = False\n",
    "truncated = False\n",
    "\n",
    "while not (termined or truncated):\n",
    "    action = 1\n",
    "    obs, reward, terminated, truncated, info  = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co zwraca środowisko?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pozycja pozioma lądownika (x);\n",
    "- pozycja pionowa lądownika (y)\n",
    "- prędkość pozioma lądownika\n",
    "- prędkość pionowa lądownika\n",
    "- kąt nachylenia lądownika\n",
    "- prędkość kątowa lądownika\n",
    "- czy noga nr 1 (lewa) lądownika dotyka podłoża\n",
    "- czy noga nr 2 (prawa) lądownika dotyka podłoża"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ile akcji może wykonać agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilość możliwych akcji: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Ilość możliwych akcji: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Brak działania\n",
    "2. Uruchomiony główny silnik\n",
    "3. Uruchomiony lewy silnik\n",
    "4. Uruchomiony peawy silnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struktura sieci głębokiej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otrzymujemy stan w postaci wektora ośmiu wyżej wymienionych parametrów. Definiujemy 3 liniowe warstwy, tzn. przetwrzające dane liniowo, realizujące sume ważoną.\n",
    "- Pierwsza warstwa przyjmuje stan w którym znajduje się łazik (state_size), rozszerza na 128 parametrów.\n",
    "- Druga warstwa przyjmuje 128 parametrów i na wyjściu ma 128 parametrów.\n",
    "- Trzecia warstwa która będzie podawać wartość Q dla każdej możliwej do podjęcia akcji przyjmuje 128 parametrów i zwraca 4 wyjścia (action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn # importujemy moduł nn z biblioteki torch\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.nn.leaky_relu(self.fc1(state), negative_slope=0.01)\n",
    "        x = torch.nn.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
    "        x = self.fc3(state)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicja agenta sieci DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque # impor kolejki deque z modułu collections\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Hyperparametry sieci neuronowej\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.input_size = input_size # ilość wejść, ilo informacji dot. stanu środowiska\n",
    "        self.output_size = output_size # ilość wyjść, ilość możliwych akcji\n",
    "        self.discount_factor = 0.99 # współczynnik dyskontujący\n",
    "        self.epsilon_greedy = 1.0 # początkowya wartość losowego wyboru akcji\n",
    "        self.epsilon_min = 0.01 # minimalna wartość losowego wyboru akcji\n",
    "        self.epsilon_greedy_decay = 0.995 # współczynnik zmniejszający wartość losowego wyboru akcji (co iteracje o 5%)\n",
    "        self.train_start = 500 # ilość próbek w pamięci agenta, po której rozpoczynamy uczenie\n",
    "        self.memory = deque(maxlen=1000) # pamięć agenta\n",
    "\n",
    "        self.model = DQNetwork(input_size, output_size).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done): \n",
    "        '''metoda dodająca informacje do pamięci agenta'''\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        '''wybiera akcję na podstawie stanu, jeśli model zwróci wartość większą niż epsilon_greedy,\n",
    "        to wybieramy akcję o największej wartości, w przeciwnym wypadku wybieramy losową akcję. unsqueeze(0) dodaje dodatkowy wymiar [BATCH_SIZE, input_size].\n",
    "        Pytorch operuje na tensorach, który w pierwszym wymiarze zawiera informacje i ilość paczek, następnie dane treningowe.\n",
    "        \"unsqueeze(0)\" dodaje dodatkowy wymiar, który jest wymagany przez model, mimo że jest jedna paczka w funkcji.'''\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon_greedy:\n",
    "            return random.randrange(self.output_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values_predicted = self.model(state)\n",
    "            return torch.argmax(q_values_predicted).item()\n",
    "\n",
    "    def replay(self):\n",
    "        '''metoda ucząca model na podstawie próbek z pamięci agenta'''\n",
    "        total_mse_loss = 0\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        data_batch = random.sample(self.memory, BATCH_SIZE) # losujemy próbki z pamięci agenta\n",
    "\n",
    "        for state, action, reward, next_state, done in data_batch:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device) #następny stan który zwróci środowisko dla danego doświdczenia\n",
    "            reward = torch.FloatTensor([reward]).to(device)\n",
    "            discount_reward = reward\n",
    "            if not done:\n",
    "                discount_reward = reward + torch.max(self.model(next_state)) * self.discount_factor # obliczamy zniżone nagrody\n",
    "            \n",
    "            dqnprediction = self.model(state)\n",
    "            true_reward = dqnprediction.clone() # klonujemy wartość predykcji\n",
    "            true_reward[action] = discount_reward # nadpisujemy wartość nagrody dla danej akcji\n",
    "\n",
    "            loss = self.loss_function(dqnprediction, true_reward)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() # aktualizujemy wagi modelu\n",
    "            total_mse_loss += loss.item()\n",
    "\n",
    "        if self.epsilon_greedy > self.epsilon_min:\n",
    "            self.epsilon_greedy *= self.epsilon_greedy_decay # zmniejszamy wartość losowego wyboru akcji\n",
    "        \n",
    "        return total_mse_loss/BATCH_SIZE # zwracamy średni błąd kwadratowy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test2137",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
